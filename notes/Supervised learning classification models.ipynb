{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you choose a classifier, please ask yourself the following questions:\n",
    "\n",
    "- Size of training dataset\n",
    "- Dimension size of the features\n",
    "- The problem is linearly separable?\n",
    "- Are features independent?\n",
    "\n",
    "Then follow the Occam's Razor principle: use the least complicated algorithm that can address your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Logistic Regression - optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- A pretty well-behaved classification algorithm that can be trained as long as you expected your features to be roughly linear  and the problem to be linearly separable. You can do some feature engineering to turn most non-linear features into linear pretty easily. \n",
    "- robut to noise and you can avoid overfitting and even do feature selection by using l2 or l1 regularization\n",
    "- Computationally inexpensive, easy update your model, pretty efficient and can be used for large dataset\n",
    "- Output can be interpreted as a proability\n",
    "- lots of ways to regularize your model and you don't have to worry about your features being correlated\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- It can hardly handle categorical (binary) features.\n",
    "- Prone to underfitting, may have low accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Step function - sigmoid\n",
    "\n",
    "$$\\sigma(z) = 1 /(1 + e ^{-z})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Optimiz the best weights (regression coefficients) - gradient descendent\n",
    "\n",
    "z = w0 * x0 + w1 * x1 + w2 * x2 + ... + wn * xn      \n",
    "In vector notation $z = (W^T*X) $\n",
    "\n",
    "\n",
    "Pseudocode for the gradient descendent:\n",
    "\n",
    "    Start with the weights all set to 1\n",
    "    Repeat R times:\n",
    "        Calculate the gradient of the entire dataset\n",
    "        update the weights vector by alpha * gradient\n",
    "        return the weight vector\n",
    "        \n",
    "Gradient descendent can be simplified with stochastic gradient descendent        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Pros and cons\n",
    "Advantages:\n",
    "\n",
    "- Effective in high dimensional spaces, especially popular in text classification problems where very high-dimensional spaces are the norm\n",
    "- Robust to noise because they maximize margins\n",
    "- make good decisions for data points that are outside the training set\n",
    "- Versatile: can model complex, nonlinear relationship different kernel functions are available\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, the methods might perform poorly\n",
    "- SVMs do not directly provide probability estimates, hard to interpret\n",
    "- Sensitive to tuning parameters and kernel choice, hard to run and tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Classes\n",
    "\n",
    "SVC and NuSVC: \n",
    "  - are similar methods but accept slightly different sets of parameters. \n",
    "  - implement the \"one-against-one\" approach for multi-class classification. n_class * (n_class - 1) / 2 classifiers are constructed \n",
    "  \n",
    "LinearSVC: \n",
    "  - Another implemetation of support vector classification for the case of a linear kernel, does not accept keyword kernel.\n",
    "  - implements \"one-vs-the-rest\" approach. trains n_class models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Scores and probabilities\n",
    "\n",
    "- the probabilities are calibrated using Platt scalling which is a expensive operation for large datasets\n",
    "- it is advisable to set probability=False and use decision_function istead of predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.Parameters\n",
    "\n",
    "- In problems where it is desired to give more importance to certain classes or certain individual samples keywords class_weight and sample_weight can be used.\n",
    "\n",
    "- C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Complexity\n",
    "\n",
    "The core of an SVM is a quadratic programmin problem (QP) so the compute and storage requirements increase rapidly with the number of training vectors. \n",
    "\n",
    "Also note that for the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. kernels - mapping data to higher dimensions\n",
    "\n",
    "One great thing about the SVM optimization is that all operations can be written in terms of inner products. We can replace the inner products with kernel functions without making simplifications. Replacing the inner product with a kernel is known as the kernel trick ot kernel substation\n",
    "\n",
    "Radial bias function (rbf, Gaussian version) kernel: $k(x,y) = exp(-||x - y||^2 / (2 * sigma^2))$\n",
    "\n",
    "There is an optimum numbert of support vectors. The beauty of SVMs is that they classify things efficiently. If you have too few support vectors, you may have a poor decision boundary. If you have too many support vectors, you're using the whole dataset every time you classify something - that's called k-Nearest Neighbors\n",
    "\n",
    "The k-Nearest Neighbors algorithm works well but you have to carry around all the training examples. With support vector machines, you can carry around far fewer example (only your support vectors) and achieve comparable performance\n",
    "\n",
    "[Multiclass SVM](https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Tips on practical use\n",
    "\n",
    "- Kernel cache size: for SVC and NuSVC,  the kernel cache has a strong impact on run times for larger problems. If you have enough RAM, it is recommended to set cache_size to a larger value\n",
    "- Setting C: C is 1 by default, If you have a lot of noisy observations, you should decrease it. \n",
    "- In SVC, if data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "- Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. \n",
    "- Use sklearn.model_selection.GridSearchCV with C and gamma spaced exponentially far apart to choose good values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Mathematical formulation\n",
    "\n",
    "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- No training involved (\"lazy\")\n",
    "- Naturally handles multiclass classification and regression\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”.\n",
    "- Expensive and slow to predict new instances\n",
    "- Must define a meaningful distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classes\n",
    "\n",
    "- KNeighborsClassifier: implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.\n",
    "- RadiusNeighborsClsffifier: implements learning based on the number of neighbors within a fixed radius r of each training point, where r is a floating-point value specified by the user. In cases where the data is not uniformly sampled, radius-based neighbors classification can be a better choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Parameters\n",
    "\n",
    "- Algorithms:\n",
    "  - Brute Force:  computation of distances between all pairs of points in the dataset: for N samples in D dimensions, this approach scales as O[D N^2]. As the number of samples N grows, the brute-force approach quickly becomes infeasible.\n",
    "  - K-D tree: the computational cost of a nearest neighbors search reduce to O[D N log(N)] or better. Though the KD tree approach is very fast for low-dimensional (D < 20) neighbors searches, it becomes inefficient as D grows very large: this is one manifestation of the so-called “curse of dimensionality”.\n",
    "  - Ball tree: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Mathematical formulation\n",
    "\n",
    "Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decision tree classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pros and cons:\n",
    "Advantages:\n",
    "\n",
    "- Simple to understand and to interpret\n",
    "- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points, computationally cheap to use\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Pseudo_code:\n",
    "\n",
    "Check if every item in the dataset is in the same class:\n",
    "\n",
    "    If so return the class label\n",
    "    Else\n",
    "        find the best feature to split the data\n",
    "        split the dataset\n",
    "        create a branch node\n",
    "            for each split \n",
    "                call breateBranch and add the result to the branch node\n",
    "            return branch node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Parameters\n",
    "\n",
    "- criterion:\n",
    "    - gini: default\n",
    "    - entropy: information gain\n",
    "\n",
    "- max_depth:\n",
    "\n",
    "- min_samples_splt: default is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Algorithm\n",
    "\n",
    "- ID3: can split norminal-valued datasets\n",
    "- C4.5\n",
    "- CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "- Efficiency, if X is a matrix of size(n,p) training has  a cost of O(knp_hat), where k is the number of iterations and p_hat is the average number of non-zero attributes per sample. It is particularly useful when the number of smples is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Tips on practical use\n",
    "\n",
    "- Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Parameters:\n",
    "\n",
    "- loss: \n",
    "    - hinge: default\n",
    "    - log: for logistic regression, a probabilistic classifier, for large dataset\n",
    "    - modified_huber: a smooth loss that brings tolerance to outliers\n",
    "    - squared_hinge: like hinge but is a quandratically penalized\n",
    "    - perceptron: a linear loss used by the perceptron algotithm\n",
    "    \n",
    "- penalty:\n",
    "    - l2: default, the standard regularizer for linear SVM models\n",
    "    - l1: bring sparsity to the model not achieveable with l2\n",
    "    - elasticnet: combination of both l1 and l2, might bring sparsity to the model not achieveable with l2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Pseudo-code:\n",
    "\n",
    "SGD is a simple yet very efficient approach. This method updates the weights using only one instance at a time to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. \n",
    "\n",
    "    Start with the weights all set to 1\n",
    "    For each pieces of data in the dataset\n",
    "        calculate the gradient of one pieces of data\n",
    "        update the weights vector by alpha * gradient\n",
    "        Return the weights vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gaussian Naive Bayes (GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Super simple\n",
    "- If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data\n",
    "- handles multiple classes\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "-  it can’t learn interactions between features \n",
    "- Sensitive to how the input data is prepared\n",
    "- Works with: norminal values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Bayes' rule:\n",
    "\n",
    "p(c|x,y) = p(x,y|c)p(c)/p(x,y)\n",
    "\n",
    "We basically compare p(c1|x,y) and p(c2|x,y):\n",
    "\n",
    "- If p(c1|x,y) > p(c2|x,y), the class is c1\n",
    "- If p(c1|x,y) < p(c2|x,y), the class is c2\n",
    "\n",
    "In naive bayes:\n",
    "\n",
    "- assume features are independent\n",
    "- assume every feature is equally important\n",
    "\n",
    "Token: is any combination of characters. You can think of tokens as words, but we may use things that aren't words such as URLs, IP address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Practical considerations \n",
    "\n",
    "- **zero probability**: We multiply a lot of probabilities together to the get the probabilities that a question belongs to a given class. If any probability is zero, then the multiplied probability will be 0. To lessen the impact of this, we  will initialize all of occurance counts to 1 and the denominators to 2.\n",
    "\n",
    "- **underflow**: We multiply a lot of probabilities together and many of these numbers are very small, we will get underflow. One solution to this is to take the natural logarithm of this product. ln(a * b) = ln(a) + ln(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Summary\n",
    "\n",
    "Using probabilities can sometimes be more effective than using hard rules for classification. Bayesian probability and Bayes' rule gives us a way to estimate unknown probabilities from known values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Two families of ensemble methods:\n",
    "\n",
    "- **Average methods:** build several estimators independently and then average their predictions.\n",
    "    - Bagging: also known as bootstrap aggregating. The data is taken from the original datasets S times to make S new dataset. The datasets are the same size as the original. each dataset is built by randomly selecting an example from the original with replacement. After the s datasets are built, a learning algorithm is applied to each dataset. When you classify a new pieces of data, you apply our S classifiers to the new piece of data and take a majority vote\n",
    "    - Forests of randomized trees\n",
    "    \n",
    "- **Boosting methods:** base estimators are built sequentially and one tries to reduce the biases of the combined estimator\n",
    "    - Adaboost： is short for adaptive boosting. Boosting applies a weight to every sample in the training data. Initially, these weights are all equal (**a weight vector D**). A weak classifier is first trained on the training data. The errors from the weak classifier are calculated, and the weak classifier is trained a second time with the same dataset. This second time the weak classifier is trained, the weights of the training set are adjusted so the examples properly classified the first time are weighted less and the examples incorrectly classified in the first iteration are weighted more. To get one answer from all of these weak classifiers, adaboost assigns alpha values to each of the classifiers. The alpha values are based on the error of each weak classifier.\n",
    "    - Gradient tree boosting\n",
    "    \n",
    "    To make this approach work, there are two fundamental questions that must be answered: first, how should each distribution be chosen on each round, and second, how should the weak rules be combined into a single rule? Regarding the choice of distribution, the technique that we advocate is to place the most weight on the examples most often misclassified by the preceding weak rules; this has the effect of forcing the base learner to focus its attention on the “hardest” examples. As for combining the weak rules, simply taking a (weighted) majority vote of their predictions is natural and effective.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ensemble Methods in Machine Learning - Thomas G. Dietterich\n",
    "[Thomas G. Dietterich](http://www.cs.orst.edu/~tgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In low noise cases. AdaBoost gives good performance because it is able to optimize the ensemble without overfitting. However, in high noise cases, AdaBoost puts a large amount of weight on the mislabeled examples and this leads it to overfit very badly__.  Bagging and Randomization do well in both the noisy and noise free cases because they are focusing on the statistical problem  and noise increases this statistical problem. \n",
    "\n",
    "In very large datasets, Randomization can be expected to do better than Bagging because bootstrap replicates of a large training set are very similar to the training set itself and hence the learned decision tree will not be very diverse. Randomization creates diversity under all conditions but at the risk of generating low quality decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Boosting\n",
    "\n",
    "error: $$\\epsilon = \\frac{number of incorrectly classified examples}{ total number of examples}$$\n",
    "\n",
    "alpha: $$\\alpha = \\frac{1}{2} (ln {\\frac{1 - \\epsilon}{\\epsilon}})$$\n",
    "\n",
    "If correctly predicted: $$D_i^{(t + 1)} =  \\frac{D_i^{(t)}e^{-\\alpha}}{Sum(D)}$$ \n",
    "If incorrectly predicted: $$D_i^{(t + 1)} =  \\frac{D_i^{(t)}e^{\\alpha}}{Sum(D)}$$\n",
    "[read here](https://www.cs.princeton.edu/courses/archive/spring07/cos424/papers/boosting-survey.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Overfitting\n",
    "\n",
    "It has been claimed in literature that for well=behaved datasets the test error for Adaboost reaches a plateau and won't increase with more classifiers. \n",
    "\n",
    "For dataset isn't \"well behaved\" , you may see that the test error reaches a minimum and then starts to increase. The dataset did start off with 30% missing values and were replaced with zeros which works well for logistic regression but they may not work for a decision tree. You can try to replace with average for a given class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Options for handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the feature's mean value from all the available data\n",
    "- Fill in the unknown with a special value like -1\n",
    "- Ignore the instance\n",
    "- Use a mean value from  similar items\n",
    "- Use another machine learning algorithm to predict the value\n",
    "\n",
    "e.g. set value as 0 work out well for logistic regression for two reasons:\n",
    "\n",
    "- weights = weights + alpha * error * dataMatrix[randindex] , if dataMatrix is 0 fro any feature, then the weight  for that feature will simply be weights\n",
    "- the error term will not be impacted because sigmoid(0) = 0.5 which is neutral for the predicting the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 9. Classification imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Alternative performance metrics: precision, recall and ROC\n",
    "\n",
    "- Confusion matrix\n",
    "- Precision: the fraction of records that were positive from the group that the classifier predicted to be positive\n",
    "$$\\frac{TP}{(TP + FP)}$$\n",
    "- Recall: the fraction of positive examples the classifier got right\n",
    "$$\\frac{TP}{(TP + FN)}$$\n",
    "\n",
    "You can easily construct a classifier that achieves a high measure of recall or precission but not both. Creating a classifier that maximize both precision and recall is a challenge\n",
    "\n",
    "- ROC curve: x-axis False positive rate; y-axis True positive rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 9.2. Manipulating the classifier's decision with a cost function\n",
    "\n",
    "original calculate the total cost: TP * 0 + FN * 1 + FP * 1 + TN * 0\n",
    "\n",
    "now consider total cost: TP * -5 + FN * 1 + FP * 50 + TN * 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Data sampling for dealing with classification imbalance\n",
    "\n",
    "\n",
    "Oversample: duplicate examples\n",
    "\n",
    "Undersample: delete examples\n",
    "\n",
    "For example, you are trying identify credit card fraud. there is a rare case. You want to preserve as much information as posible about the rare case. So you should keep all of the examples from the positive class and undersample or discard examples from the negative class. One draback of this approach is deciding which negative examples to toss out. The examples you choose to toss out could carry valuable information that isn't contained in the remaining examples. One solution for this is to pick samples to discard that aren't near the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
