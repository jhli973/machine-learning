{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you choose a classifier, please ask your self the following questions:\n",
    "\n",
    "- Size of training dataset\n",
    "- Dimension size of the features\n",
    "- The problem is linearly separable?\n",
    "- Are features independent?\n",
    "\n",
    "Then follow the Occam's Razor principle: use the least complicated algorithm tha can address your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- A pretty well-behaved classification algorithm that can be trained as long as you expected your features to be roughly linear  and the problem to be linearly separable. You can do some feature engineering to turn most non-linear features into linear pretty easily. \n",
    "- robut to noise and you can aviod overfitting and even do feature selection by using l2 or l1 regularization\n",
    "- easily update your model, pretty efficient and can be used for large dataset\n",
    "- Output can be interpreted as a proability\n",
    "- lots of ways to regularize your model and you don't have to worry about your features being correlated\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- it can hardly handle categorical (binary) features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Pros and cons\n",
    "Advantages:\n",
    "\n",
    "- Effective in high dimensional spaces, especially popular in text classification problems where very high-dimensional spaces are the norm\n",
    "- Robust to noise because they maximize margins\n",
    "- Versatile: can model complex, nonlinear relationship different kernel functions are available\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- If the number of features is much greater than the number of samples, the methods might perform poorly\n",
    "- SVMs do not directly provide probability estimates, hard to interpret\n",
    "- Memory-intensive\n",
    "- hard to run and tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Classes\n",
    "\n",
    "SVC and NuSVC: \n",
    "  - are similar methods but accept slightly different sets of parameters. \n",
    "  - implement the \"one-against-one\" approach for multi-class classification. n_class * (n_class - 1) / 2 classifiers are constructed \n",
    "  \n",
    "LinearSVC: \n",
    "  - Another implemetation of support vector classification for the case of a linear kernel, does not accept keyword kernel.\n",
    "  - implements \"one-vs-the-rest\" approach. trains n_class models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Scores and probabilities\n",
    "\n",
    "- the probabilities are calibrated using Platt scalling which is a expensive operation for large datasets\n",
    "- it is advisable to set probability=False and use decision_function istead of predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.Parameters\n",
    "\n",
    "- In problems where it is desired to give more importance to certain classes or certain individual samples keywords class_weight and sample_weight can be used.\n",
    "\n",
    "- C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Complexity\n",
    "\n",
    "The core of an SVM is a quadratic programmin problem (QP) so the compute and storage requirements increase rapidly with the number of training vectors. \n",
    "\n",
    "Also note that for the linear case, the algorithm used in LinearSVC by the liblinear implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Tips on practical use\n",
    "\n",
    "- Kernel cache size: for SVC and NuSVC,  the kernel cache has a strong impact on run times for larger problems. If you have enough RAM, it is recommended to set cache_size to a larger value\n",
    "- Setting C: C is 1 by default, If you have a lot of noisy observations, you should decrease it. \n",
    "- In SVC, if data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "- Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. \n",
    "- Use sklearn.model_selection.GridSearchCV with C and gamma spaced exponentially far apart to choose good values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Mathematical formulation\n",
    "\n",
    "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- No training involved (\"lazy\")\n",
    "- Naturally handles multiclass classification and regression\n",
    "\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”.\n",
    "- Expensive and slow to predict new instances\n",
    "- Must define a meaningful distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Classes\n",
    "\n",
    "- KNeighborsClassifier: implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.\n",
    "- RadiusNeighborsClsffifier: implements learning based on the number of neighbors within a fixed radius r of each training point, where r is a floating-point value specified by the user. In cases where the data is not uniformly sampled, radius-based neighbors classification can be a better choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Parameters\n",
    "\n",
    "- Algorithms:\n",
    "  - Brute Force:  computation of distances between all pairs of points in the dataset: for N samples in D dimensions, this approach scales as O[D N^2]. As the number of samples N grows, the brute-force approach quickly becomes infeasible.\n",
    "  - K-D tree: the computational cost of a nearest neighbors search reduce to O[D N log(N)] or better. Though the KD tree approach is very fast for low-dimensional (D < 20) neighbors searches, it becomes inefficient as D grows very large: this is one manifestation of the so-called “curse of dimensionality”.\n",
    "  - Ball tree: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Mathematical formulation\n",
    "\n",
    "Neighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Decision tree classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pros and cons:\n",
    "Advantages:\n",
    "\n",
    "- Simple to understand and to interpret\n",
    "- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Parameters\n",
    "\n",
    "- criterion:\n",
    "    - gini: default\n",
    "    - entropy: information gain\n",
    "\n",
    "- max_depth:\n",
    "\n",
    "- min_samples_splt: default is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "- Efficiency, if X is a matrix of size(n,p) training has  a cost of O(knp_hat), where k is the number of iterations and p_hat is the average number of non-zero attributes per sample. It is particularly useful when the number of smples is very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Tips on practical use\n",
    "\n",
    "- Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Parameters:\n",
    "\n",
    "- loss: \n",
    "    - hinge: default\n",
    "    - log: for logistic regression, a probabilistic classifier, for large dataset\n",
    "    - modified_huber: a smooth loss that brings tolerance to outliers\n",
    "    - squared_hinge: like hinge but is a quandratically penalized\n",
    "    - perceptron: a linear loss used by the perceptron algotithm\n",
    "    \n",
    "- penalty:\n",
    "    - l2: default, the standard regularizer for linear SVM models\n",
    "    - l1: bring sparsity to the model not achieveable with l2\n",
    "    - elasticnet: combination of both l1 and l2, might bring sparsity to the model not achieveable with l2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4\n",
    "\n",
    "SGD is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Gaussian Naive Bayes (GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Pros and cons\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Super simple\n",
    "- If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "-  it can’t learn interactions between features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Two families of ensemble methods:\n",
    "\n",
    "- **Average methods:** build several estimators independently and then average theri predictions.\n",
    "    - Bagging\n",
    "    - Forests of randomized trees\n",
    "    \n",
    "- **Boosting methods:** base estimators are built sequentially and one tries to reduce the biases of the combined estimator\n",
    "    - Adaboost\n",
    "    - Gradient tree boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ensemble Methods in Machine Learning - Thomas G. Dietterich\n",
    "[Thomas G. Dietterich](http://www.cs.orst.edu/~tgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In low noise cases. AdaBoost gives good performance because it is able to optimize the ensemble without overfitting However, in high noise cases, AdaBoost puts a large amount of weight on the mislabeled examples and this leads it to overfit very badly.  Bagging and Randomization do well in both the noisy and noise free cases because they are focusing on the statistical problem  and noise increases this statistical problem. \n",
    "\n",
    "In very large datasets, Randomization can be expected to do better than Bagging because bootstrap replicates of a large training set are very similar to the training set itself and hence the learned decision tree will not be very diverse Randomization creates diversity under all conditions but at the risk of generating low quality decision trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
