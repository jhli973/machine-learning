{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from [sklearn](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models\n",
    "\n",
    "### 1. LinearRegression\n",
    "\n",
    "Fits a linear model with coefficients w = (w_1, ..., w_p) to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation.\n",
    "\n",
    "LinearRegression will take in its fit method arrays X, y and will store the coefficients w of the linear model in its coef_\n",
    "\n",
    "However, coefficient estimates for Ordinary Least Squares rely on the independence of the model terms. When terms are correlated and the columns of the design matrix X have an approximate linear dependence, the design matrix becomes close to singular and as a result, the least-squares estimate becomes highly sensitive to random errors in the observed response, producing a large variance. This situation of multicollinearity can arise, for example, when data are collected without an experimental design.\n",
    "\n",
    "If X is a matrix of size (n, p) this method has a cost of O(n p^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ridge Regression\n",
    "\n",
    "Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated). Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares.\n",
    "\n",
    "Alpha is a complexity parameter that controls the amount of shrinkage: the larger the value of alpha, the greater the amount of shrinkage and thus the coefficients become more robust to collinearity. If alpha is large it means model is less complex. So in this case bias is high and variance in low. Specifically, we can see that when alpha is 0, we get our least square solution (Linear regression). When alpha goes to infinity, we get very, very small coefficients approaching 0.\n",
    "\n",
    "The model complexity has same order of complexity as linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lasso (Least Absolute Shrinkage and Selection Operator) \n",
    "\n",
    "The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent. The Lasso does not admit a closed-form solution. The L1-penalty makes the solution non-linear. If group of predictors are highly correlated, lasso picks only one of them and shrinks the others to zero\n",
    "\n",
    "The lasso estimate thus solves the minimization of the least-squares penalty with alpha ||w||_1 added, where alpha is a constant and ||w||_1 is the \\ell_1-norm of the parameter vector.\n",
    "\n",
    "The alpha parameter controls the degree of sparsity of the coefficients estimated.\n",
    "\n",
    "scikit-learn exposes objects that set the Lasso alpha parameter by cross-validation: LassoCV and LassoLarsCV. LassoLarsCV is based on the Least Angle Regression algorithm. For high-dimensional datasets with many collinear regressors, LassoCV is most often preferable. However, LassoLarsCV has the advantage of exploring more relevant values of alpha parameter, and if the number of samples is very small compared to the number of observations, it is often faster than LassoCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elastic Net\n",
    "\n",
    "ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. We control the convex combination of L1 and L2 using the l1_ratio parameter. \n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both. A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridgeâ€™s stability under rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Least Angle Regression (LARS)\n",
    "\n",
    "Least-angle regression (LARS) is a regression algorithm for high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani.\n",
    "\n",
    "The advantages of LARS are:\n",
    "- It is numerically efficient in contexts where p >> n (i.e., when the number of dimensions is significantly greater than the number of points)\n",
    "- It is computationally just as fast as forward selection and has the same order of complexity as an ordinary least squares.\n",
    "- It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model.\n",
    "- If two variables are almost equally correlated with the response, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.\n",
    "- It is easily modified to produce solutions for other estimators, like the Lasso.\n",
    "\n",
    "The disadvantages of the LARS method include:\n",
    "\n",
    "- Because LARS is based upon an iterative refitting of the residuals, it would appear to be especially sensitive to the effects of noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayesian Regression\n",
    "\n",
    "Bayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. Alpha is again treated as a random variable that is to be estimated from the data.\n",
    "\n",
    "The advantages of Bayesian Regression are:\n",
    "- It adapts to the data at hand.\n",
    "- It can be used to include regularization parameters in the estimation procedure.\n",
    "\n",
    "The disadvantages of Bayesian regression include:\n",
    "- Inference of the model can be time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large. The partial_fit method allows only/out-of-core learning.\n",
    "\n",
    "The classes  SGDClassifier and  SGDRegressor provide functionality to fit linear models for classification and regression using different (convex) loss functions and different penalties. E.g., with loss=\"log\", SGDClassifier fits a logistic regression model, while with loss=\"hinge\" it fits a linear support vector machine (SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree\n",
    "\n",
    "\n",
    "A non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Some advantages of decision trees are:\n",
    "- Simple to understand and to interpret. Trees can be visualised.\n",
    "- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "- Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n",
    "- Able to handle multi-output problems.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "scikit-learn uses an optimised version of the CART algorithm. CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensemble - GradientBoostingRegressor (GBRT)\n",
    "\n",
    "supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares ('ls')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. [45 questions to test a Data Scientist on Regression (Skill test â€“ Regression Solution)](https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What are the assumptions made in regression ? from [here](http://blog.hackerearth.com/beginners-guide-to-regression-analysis-plot-interpretations.html) and [here](https://www.analyticsvidhya.com/blog/2016/07/deeper-regression-analysis-assumptions-plots-solutions/)\n",
    "\n",
    "Regression is a parametric technique:\n",
    "\n",
    "- There should be a linear and additive relationship between dependent (DV) and independent variables (IV). By linear, it means that the change in DV by 1 unit change in IV is constant. By additive, it refers to the effect of X on Y is independent of other variables.\n",
    "- There must be no correlation among independent variables (scatter plot to visualize correlation effect among variables. Also, you can also use VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. a correlation table should also solve the purpose.). Presence of correlation in independent variables lead to Multicollinearity. If variables are correlated, it becomes extremely difficult for the model to determine the true effect of IVs on DV.\n",
    "- The residual (error) terms must possess constant variance. Absence of constant variance leads to heteroskedestacity (look at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern ).\n",
    "- The residual (error) terms must be uncorrelated (check QQ). Presence of correlation in error terms is known as Autocorrelation. It drastically affects the regression coefficients and standard error values since they are based on the assumption of uncorrelated error terms.\n",
    "- The dependent variable and the error terms must possess a normal distribution.\n",
    "\n",
    "Presence of these assumptions make regression quite restrictive. By restrictive I meant, the performance of a regression model is conditioned on fulfillment of these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### How can you improve the accuracy of a regression model ?\n",
    "\n",
    "There is little you can do when your data violates regression assumptions. An obvious solution is to use tree-based algorithms which capture non-linearity quite well. But if you are adamant at using regression, following are some tips you can implement:\n",
    "\n",
    "- If your data is suffering from non-linearity (Look for residual vs fitted value plots), transform the IVs using sqrt, log, square, etc.\n",
    "- If your data is suffering from heteroskedasticity, transform the DV using sqrt, log, square, etc. Also, you can use weighted least square method to tackle this problem.\n",
    "- If your data is suffering from multicollinearity, use a correlation matrix to check correlated variables. Let's say variables A and B are highly correlated. Now, instead of removing one of them, use this approach: Find the average correlation of A and B with the rest of the variables. Whichever variable has the higher average in comparison with other variables, remove it. Alternatively, you can use penalized regression methods such as lasso, ridge, elastic net, etc.\n",
    "- You can do variable selection based on p values. If a variable shows p value > 0.05, we can remove that variable from model since at p> 0.05, we'll always fail to reject null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### About  heteroskedasticity\n",
    "\n",
    "The presense of non-constant variance in the error terms results in heteroskedasticity. Generally, non-constant variance arises because of presense of outliers or extreme leverage values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About R square\n",
    "\n",
    "Adding a variable in a linear regression model, R square will always increase or stays constant but it is not true in case of adjusted R square. If it increases, the feature would be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics used for evaluating regression models\n",
    "R Squared, Adjusted R Squared, F Statistics , RMSE / MSE / MAE are some metrics which you can use to evaluate your regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Correlation\n",
    "\n",
    "- Correlation is a statistic metric that measures the linear association between two variables. It treats y and x symmetrically.\n",
    "- Regression is setup to predict y from x. The relationship is not symmetric. linear regression gives much more information about the relationship than Pearson Correlation. The slope in  a linear regression gives the marginal change in output/target variable by changing the independent variable by unit distance. Correlation has no slope.\n",
    "The intercept in a linear regression gives the value of target variable if one of the input/independent variable is set zero. Correlation does not have this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between ridge and regression and lasso, please refer [here](https://discuss.analyticsvidhya.com/t/difference-between-ridge-regression-and-lasso-and-its-effect/3000)\n",
    "\n",
    "Ridge and Lasso regression uses two different penalty functions. Ridge uses l2 where as lasso go with l1. In ridge regression, the penalty is the sum of the squares of the coefficients and for the Lasso, it's the sum of the absolute values of the coefficients. It's a shrinkage towards zero using an absolute value (l1 penalty) rather than a sum of squares(l2 penalty).\n",
    "\n",
    "As we know that ridge regression can't zero coefficients. Here, you either select all the coefficients or none of them whereas LASSO does both parameter shrinkage and variable selection automatically because it zero out the co-efficients of collinear variables. Here it helps to select the variable(s) out of given n variables while performing lasso regression.\n",
    "\n",
    "Another type of regularization method is ElasticNet, it is hybrid of lasso and ridge regression both. It is trained with L1 and L2 prior as regularizer. A practical advantage of trading-off between Lasso and Ridge is that, it allows Elastic-Net to inherit some of Ridgeâ€™s stability under rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points:\n",
    "\n",
    "- There must be linear relationship between independent and dependent variables\n",
    "- Multiple regression suffers from multicollinearity, autocorrelation, heteroskedasticity.\n",
    "- Linear Regression is very sensitive to Outliers. It can terribly affect the regression line and eventually the forecasted values.\n",
    "- Multicollinearity can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is that the coefficient estimates are unstable\n",
    "- In case of multiple independent variables, we can go with **forward selection, backward elimination and step wise approach** for selection of most significant independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
